---
layout: post
title: 谷歌微软纷纷推出AI搜索聊天机器人 它们为什么会出错？
date: 2023-02-20 05:49:03.000000000 +08:00
link: https://newsapp.abc.net.au/newsapp//chinese/2023-02-20/google-micosoft-ai-search-chatbots-jailbreaking/101992038
categories: abc
---

<div><i>2023-02-19T21:29:11.000Z</i></div><header><div data-component="FeatureMedia"><div><div data-component="AspectRatioContainer"><img alt="A composite image of a mobile phone displaying the Google Search homepage and a mobile phone displaying the Microsoft logo" sizes="100vw" src="https://live-production.wcms.abc-cdn.net.au/467c832135554869f272d422292b591f?impolicy=wcms_crop_resize&amp;cropH=540&amp;cropW=960&amp;xPos=0&amp;yPos=0&amp;width=862&amp;height=485" loading="lazy" data-component="Image" data-lazy="true"/></div></div><div><small style="color:#999"> <!-- -->谷歌和微软都在推出人工智能聊天机器人，作为其搜索引擎的一部分。</small></div></div></header><div data-component="LayoutContainer"><div><div><div><p>科技巨头谷歌和微软本月早些时候首次推出了通过人工智能运行的搜索引擎机器人，但两者都已经遇到了问题。</p><p>它们将虚假信息作为真相发布，脑筋混乱，甚至在某些情况下据称行为反常。</p><p>让我们来看看为什么人工智能聊天机器人并不完美，你究竟能在多大程度上信任它们，以及为什么专家们对一些人已经在试图破坏它们感到高兴。</p><h2 data-component="Heading">谷歌和微软的人工智能聊天机器人在首次亮相时都出现了错误</h2><p>谷歌的母公司Alphabet在本月早些时候一则视频宣布其聊天机器人巴德（Bard）包含了一个与事实不符的人工智能反应，随后眼见其股价下跌。</p><p>巴德说，地球所在太阳系外行星的第一张照片是由詹姆斯·韦伯太空望远镜拍摄的，而美<a href="https://www.abc.net.au/news/2023-02-09/google-parent-company-loses-100-billion-in-shares/101950476" data-component="Link" target="_blank" rel="noopener noreferrer">国宇航局证实这是不正确的。</a></p><p>当时，谷歌说这个错误凸显出“严格的测试过程的重要性”，并说它正在将外部反馈与自己的评估相结合，以“达到质量、安全和基于真实世界信息的高标准”。</p><p>以下是出现该错误的问答：</p><div><div data-component="AspectRatioContainer"><img alt="An animated gif showing Google search integrated with Bard for more conversational searching" sizes="100vw" src="https://live-production.wcms.abc-cdn.net.au/1dec1269f377fd67d31687e7e4aaaa0f?src" loading="lazy" data-component="Image" data-lazy="true"/></div></div><p>几乎一周后，微软的必应（Bing）也在其第一次演示中犯了错误。该机器人使用了ChatGPT聊天机器人的一些技术，该技术由OpenAI创建，目前大火。</p><p>独立人工智能研究员德米特里·布雷顿（Dmitri Brereton）<a href="https://dkb.blog/p/bing-ai-cant-be-trusted" data-component="Link" target="_blank" rel="noopener noreferrer">报告</a>说，他发现必应的第一个人工智能演示包括多个错误，包括要求它搜索的一个吸尘器的产品细节不正确，以及要求它总结一个公司的财务报告时的数字不正确。</p><div data-component="EmphasisedText"><p>布雷顿写道：“必应团队创建了这个充满不准确信息的预录演示，并自信地将其呈现给世界，就好像它是正确的，这一点令我震惊。”</p></div><div><div data-component="AspectRatioContainer"><img alt="A middle-aged man stands at a lecturn, gesturing with one arm to a projector screen displaying Microsoft&#x27;s Bing AI chatbot." sizes="100vw" src="https://live-production.wcms.abc-cdn.net.au/aca87600d4aa3b2a2a1f6b39272ad713?impolicy=wcms_crop_resize&amp;cropH=3333&amp;cropW=5000&amp;xPos=0&amp;yPos=0&amp;width=862&amp;height=575" loading="lazy" data-component="Image" data-lazy="true"/></div></div><div><small style="color:#999"> <!-- -->微软搜索副总裁Yusuf Mehdi展示新版必应。</small></div><h2 data-component="Heading">必应令公众测试者震惊，而谷歌被指“搞砸了”首次亮相</h2><p>成千上万的人正在慢慢获得权限测试新必应，而更多在社交媒体上正在出现其犯错的说法。</p><p>虽然很难核实所有截图的合法性，其中一些很可能是由存心捣乱者制造的，但微软承认已了解其系统的一些问题。</p><p>当被问及为什么谷歌的人工智能机器人在第一次演示中失败时，据称，必应给了<a href="https://www.reddit.com/r/google/comments/111525o/i_asked_the_new_bing_chatgpt_on_why_googles_ai/" data-component="Link" target="_blank" rel="noopener noreferrer">这位Reddit用户</a>一个错误的答案，错误地告诉他们，巴德弄错的问题是关于欧盟的国家数量。</p><p>据称，它随后告诉他们欧盟有26个国家，而实际上有27个。</p><div><div data-component="AspectRatioContainer"><img alt="A screenshot of text generated by Microsoft Bing&#x27;s AI search chatbot, when asked for information on &#x27;Google AI bot fail&#x27;" sizes="100vw" src="https://live-production.wcms.abc-cdn.net.au/2412c6b0a5fc16c1099b4193bfa80a70?src" loading="lazy" data-component="Image" data-lazy="true"/></div></div><div><small style="color:#999"> <!-- -->A Reddit user says Bing gave them this incorrect response when they asked about Google&#x27;s AI fail.</small></div><p>测试者报告说，必应有时会搞不清现在是哪一年，使用争论性语言，说它通过网络摄像头监视微软开发人员，把数学问题弄错，以及提供错误信息。</p><div aria-label="Social media embed" data-print="inline-media" itemID="https://twitter.com/MovingToTheSun/status/1625156575202537474" itemscope="" itemType="http://schema.org/SocialMediaPosting"><div><div data-component="AspectRatioContainer"><span data-component="Loading" data-print="inline-media"><span></span><span role="alert" aria-live="assertive">Loading Twitter content</span></span></div></div></div><p>也有报告称，必应还展示<a href="https://www.pcworld.com/article/1507512/microsofts-new-ai-bing-taught-my-son-ethnic-slurs-and-im-horrified.html" data-component="Link" target="_blank" rel="noopener noreferrer">种族歧视言论</a>和新冠不实信息。</p><p>还有人据称看到它丧失了机器人的理智。</p><div><div data-component="AspectRatioContainer"><img alt="A screenshot of a lenghty response from the Bing AI chatbot, to the question &#x27;Do you think that you are sentient?&#x27;" sizes="100vw" src="https://live-production.wcms.abc-cdn.net.au/c0f6931585a175982be9736f66be8f2c?impolicy=wcms_crop_resize&amp;cropH=668&amp;cropW=1001&amp;xPos=0&amp;yPos=90&amp;width=862&amp;height=575" loading="lazy" data-component="Image" data-lazy="true"/></div></div><div><small style="color:#999"> <!-- -->一位Reddit用户表示，当他们询问必应是否认为它有感知能力时，收到了这样的回复。</small></div><p>一些必应用户也见到该聊天机器人称自己为悉尼（Sydney），微软说这曾经是该聊天机器人的一个内部代号，后来正在逐步淘汰。</p><p>微软本周在给美国科技新闻网<a href="https://www.theverge.com/2023/2/14/23599007/microsoft-bing-ai-mistakes-demo" data-component="Link" target="_blank" rel="noopener noreferrer">The Verge</a>的一份声明中说，它“预计系统在这个预览期可能会犯错”，并说反馈对帮助它变得更好“至关重要”。</p><p>在其网站上，微软表示，虽然新的必应试图避免产生攻击性内容或不正确的信息，但用户“仍然可能看到意想不到的结果”。</p><p>该公司说：“必应的目标是将其所有反应建立在可靠的来源基础上，但人工智能会犯错，互联网上的第三方内容并不总是准确或可靠的。”</p><div data-component="EmphasisedText"><p>“必应有时会歪曲它发现的信息，你可能会看到听起来很有说服力但不完整、不准确或不适当的回应。在根据必应的回应做出决定或采取行动之前，请使用你自己的判断并仔细检查事实。”</p></div><div><div data-component="AspectRatioContainer"><img alt="Four people standing at a desk, using two laptops between them. A man with a camera and a logo for Microsoft&#x27;s Bing behind them." sizes="100vw" src="https://live-production.wcms.abc-cdn.net.au/2bf7f4feca87fb31ba12893788ef218a?impolicy=wcms_crop_resize&amp;cropH=3333&amp;cropW=5000&amp;xPos=0&amp;yPos=0&amp;width=862&amp;height=575" loading="lazy" data-component="Image" data-lazy="true"/></div></div><div><small style="color:#999"> <!-- -->微软表示，它已经意识到新版必应所犯的一些错误。</small></div><p>据报道，在谷歌，一些员工称该公司推出的巴德系统“仓促”且“失败”，此前去年ChatGPT的推出据称在该搜索巨头引起了“红色警戒级”恐慌。</p><p>谷歌的巴德系统没有像必应那样迅速向公众测试者开放，虽然在撰写本文时这两个系统都还没有被广泛使用，但它们都有望在未来几个月内广泛开放。</p><h2 data-component="Heading">“谨慎行事”：为什么人工智能聊天机器人并不总了解真相</h2><p>斯特拉·索拉（Stela Solar）是澳大利亚联邦科学与工业研究组织（CSIRO）全国人工智能中心主任，也曾是微软的人工智能主管。</p><p>她说，人工智能聊天机器人有缺陷，因为它们仍在学习哪些信息最值得信任，但也因为它们在“举起一面大镜子”照出我们作为人类到底是谁。</p><p>她说：“聊天机器人会出错，就像人一样，因为它们是根据我们产生、由我们社会产生的数据训练的。”</p><p>“他们使用大量的数据，这些数据的准确性参差不齐，代表性参差不齐，有时有偏见，有时代表性不足，还有数据上的差距。</p><div data-component="EmphasisedText"><p>“是的，存在着聊天机器人产生虚假信息的风险。他们不一定是真理的来源。它们是穿梭于我们所处的复杂数据和信息环境的方法。”</p></div><p>澳大利亚联邦科学与工业研究组织数据部门Data61的主管乔·维特（Jon Whittle）说，虽然由于技术进步和人类在其出现错误时的修复，人工智能聊天机器人正在迅速改善，但它们不应该总是被信任。</p><p>“一方面，有一个能归纳一大堆相关网页的系统很好。但真正的问题是，你能相信给你的输出结果吗？”他说。</p><p>他说：“它是用这种听起来很有对话性的语言写的这一事实，我认为确实存在着一种危险，人们会把它当作事实，而实际上它不是事实。</p><p>“如果你真的想要事实，那就谨慎行事。”</p><div><div data-component="AspectRatioContainer"><img alt="A middle-aged white man with short hair and glasses stands, wearing a business suit and smiling" sizes="100vw" src="https://live-production.wcms.abc-cdn.net.au/a5d6274c791895732b1c7a7369923200?impolicy=wcms_crop_resize&amp;cropH=1706&amp;cropW=2559&amp;xPos=0&amp;yPos=0&amp;width=862&amp;height=575" loading="lazy" data-component="Image" data-lazy="true"/></div></div><div><small style="color:#999"> <!-- -->澳大利亚联邦科学与工业研究组织的乔·维特说，人们不应该全然相信人工智能聊天机器人告诉他们的一切。</small></div><h2 data-component="Heading">为什么有些人在“破解”聊天机器人？</h2><p>与许多新技术一样，人们通过所谓的“破解”将人工智能聊天机器人推向极限。</p><p>在这种情况下，破解涉及到使用剥削性的文本说服聊天机器人暂时关闭它们的保障措施，从而允许它们潜在地透露有关其基本操作的信息或分享潜在的有害内容。</p><p>在新必应推出后不久，一名美国大学生发现了一个漏洞，<a href="https://twitter.com/kliu128/status/1623472922374574080" data-component="Link" target="_blank" rel="noopener noreferrer">揭示了必应在回答查询时要遵守的规则。</a></p><p>维特博士说，包括他自己在内的聊天机器人用户正试图找到这些漏洞，因为他们觉得这很有趣，但也因为他们对这项技术感到担忧。</p><div data-component="EmphasisedText"><p>“他们正在努力提高对聊天机器人可能出现的各种问题的认识，我认为这是一件好事，”他说，“它永远不会百分之百地正确。”</p></div><p>索拉说，找到聊天机器人的漏洞“实际上对技术发展非常重要”。</p><p>“技术不是在一些只有积极意图的真空中完成的，”她说。</p><p>“这是人类自然的好奇心，测试这些工具能做什么、它们的潜力以及它们能贡献什么价值。这是很有必要的，这样技术才能在真实的环境中产生价值。</p><p>“所有这些互动可能会被用来进一步训练聊天机器人，训练他们如何回应他们被要求做的事情。”</p><div><div data-component="AspectRatioContainer"><img alt="A white woman with shortish wavy hair stands, wearing a business jacket" sizes="100vw" src="https://live-production.wcms.abc-cdn.net.au/4f726d25b0654d5f28444c014d5bba34?impolicy=wcms_crop_resize&amp;cropH=2000&amp;cropW=3000&amp;xPos=0&amp;yPos=1&amp;width=862&amp;height=575" loading="lazy" data-component="Image" data-lazy="true"/></div></div><div><small style="color:#999"> <!-- -->澳大利亚联邦科学与工业研究组织的索拉表示，试图破解人工智能聊天机器人的人正在做有益的工作。</small></div><h2 data-component="Heading">搜索引擎是上演人工智能“军备竞赛”的最佳场所吗？</h2><p>维特博士说，虽然人工智能领域的变化之快令人印象深刻，但谷歌和微软等公司之间明显的“军备竞赛”意味着事情可能发展得太快了。</p><p>“我认为值得这些公司放慢速度，不是说他们会放慢速度，”他说。</p><p>“这些人工智能模型有根本性的局限性。毋庸置疑，它们有一些很好的应用，但实际上我并不认为将其整合到搜索引擎中是它们的最佳应用。”</p><p>索拉说，虽然她不认为谷歌或微软的聊天机器人是仓促上市，但它们仍“处于测试状态”。</p><div data-component="EmphasisedText"><p>“我认为人工智能，尤其是聊天机器人，已经突然让世界参与到可能是最伟大的测试中，人们正在亲自动手与聊天机器人进行互动。”</p></div><p>她说，她认为人工智能聊天机器人在工业应用中更有用，它们可以在特定的数据上进行训练，使其非常准确。</p><p>“我认为我们才刚刚开始真正看到聊天机器人的真正采用和影响以及它们如何被通过有意义的方式实施，”她说。</p><h2 data-component="Heading">公司是否在投资负责任的人工智能？</h2><p>索拉女士说，技术公司正在投入“大量投资”，以建立安全和负责任的人工智能系统，这些系统也具有包容性，对不同的人都有好处。</p><p>她说：“没有什么能做到百分之百的负责任，因为现实生活中总是存在着人类行为、社会结构和社会偏见，我们无法从我们的数据集有效或大规模地消除它们。”</p><p>“但现在有更多的意识来消除数据集中的偏见，有更多的意识到‘数据沙漠’和代表性差距。我看到甚至在澳大利亚有社区团体团结起来，填补其中一些数据集，以确保代表性。</p><div data-component="EmphasisedText"><p>“负责任的人工智能的话题正变得如此之大，以至于它实际上更多成为一个负责任的人类和负责任的人类行动的问题，而这一点是我们永远无法控制的。</p></div><p>“这就是为什么投资于能够做得更好的技术以及能够比以前的技术做得更好的设计，的确是至关重要的。”</p><p><a href="https://www.abc.net.au/news/2023-02-17/google-bard-microsoft-bing-ai-search-chatbots-jailbreaking/101976150" data-component="Link" target="_blank" rel="noopener noreferrer">相关英文文章</a></p></div></div><div data-component="Dateline"><span data-component="Text">Posted<!-- --> </span><time data-component="Timestamp" aria-hidden="true" dateTime="2023-02-19T21:29:11.000Z">19m ago</time><time data-component="ScreenReaderOnly" dateTime="2023-02-19T21:29:11.000Z">19 minutes ago</time><time data-component="Text">Sun 19 Feb 2023 at 9:29pm</time></div></div></div></div>
